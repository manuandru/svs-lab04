{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carla 03\n",
    "\n",
    "## Conda Setup\n",
    "\n",
    "Please run the following from `Anaconda Powershell Prompt` (or any other terminal with `conda` installed).\n",
    "\n",
    "From Lab PC, you can find it in the start menu or in the Desktop.\n",
    "\n",
    "### 1. Environment creation\n",
    "\n",
    "Create a new environment with the following command (you can skip this step if you already have a conda environment):\n",
    "\n",
    "```bash\n",
    "conda create -n carla-env python=3.7\n",
    "```\n",
    "\n",
    "### 2. Environment activation\n",
    "\n",
    "Activate the environment with the following command:\n",
    "\n",
    "```bash\n",
    "conda activate carla-env\n",
    "```\n",
    "\n",
    "### 3. Package installation\n",
    "\n",
    "Install the required packages with the following command (run it where the `requirements.txt` file is located):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 4. VSCode setup\n",
    "\n",
    "If you are using VSCode, you can select the conda environment by clicking on the bottom left corner:\n",
    "\n",
    "`Select kernel > Python Environments > carla-env`\n",
    "\n",
    "Let vscode install the required packages for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla, time, pygame, math, random, cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "\n",
    "world = client.get_world()\n",
    "spectator = world.get_spectator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helpful functions used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_spectator_to(transform, distance=5.0, x=0, y=0, z=4, yaw=0, pitch=-30, roll=0):\n",
    "    back_location = transform.location - transform.get_forward_vector() * distance\n",
    "    \n",
    "    back_location.x += x\n",
    "    back_location.y += y\n",
    "    back_location.z += z\n",
    "    transform.rotation.yaw += yaw\n",
    "    transform.rotation.pitch = pitch\n",
    "    transform.rotation.roll = roll\n",
    "    \n",
    "    spectator_transform = carla.Transform(back_location, transform.rotation)\n",
    "    \n",
    "    spectator.set_transform(spectator_transform)\n",
    "\n",
    "def spawn_vehicle(vehicle_index=0, spawn_index=0, pattern='vehicle.*'):\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_bp = blueprint_library.filter(pattern)[vehicle_index]\n",
    "    spawn_point = world.get_map().get_spawn_points()[spawn_index]\n",
    "    vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "    return vehicle\n",
    "\n",
    "def draw_on_screen(world, transform, content='O', color=carla.Color(0, 255, 0), life_time=20):\n",
    "    world.debug.draw_string(transform.location, content, color=color, life_time=life_time)\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=1.2, z=1.2), carla.Rotation(pitch=-10)), width=800, height=600):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDE suggestions\n",
    "\n",
    "In VSCode, you can get suggestions for functions and classes by pressing `Ctrl + Space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = world.get_map()\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, carla does not provide docs for the functions and classes, so we need to install it manually.\n",
    "\n",
    "It can be done in VSCode with the following the instructions (resumed below):\n",
    "\n",
    "1. Go to the repo https://github.com/aasewold/carla-python-stubs\n",
    "2. Go to the releases section\n",
    "3. Download `*.pyi` file (i.e `__init__.pyi` and `command.pyi`)  that match carla version (e.g 0.9.15)\n",
    "4. Put the file in `./typings/carla` ([vscode reference](https://code.visualstudio.com/docs/python/settings-reference))\n",
    "\n",
    "> **Note**: `./typings/carla` should be relative to the root of the vscode project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensors\n",
    "\n",
    "Sensors are another type of actors, usually spawned attached to a vehicle, designed to retrieve data from the world. The type of data depends on the sensor, but it can range from RGB images, LiDAR scans or even collision information.\n",
    "\n",
    "Sensors are divided into two main categories:\n",
    "\n",
    "- **Regular sensors**: these sensors retrieve data at a fixed rate, usually every tick (e.g RGB cameras).\n",
    "- **Trigger sensors**: these sensors only retrieve data when a certain condition is met (e.g collision sensors).\n",
    "\n",
    "Sensors details can be found in the [CARLA documentation](https://carla.readthedocs.io/en/latest/ref_sensors/).\n",
    "\n",
    "![Sensors](img/sensors.jpg)\n",
    "\n",
    "### Exploring sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = world.get_blueprint_library().filter('sensor.*')\n",
    "\n",
    "for sensor in sensors:\n",
    "    print(sensor.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "\n",
    "camera_bp.set_attribute('image_size_x', '1280')\n",
    "camera_bp.set_attribute('image_size_y', '720')\n",
    "camera_bp.set_attribute('fov', '120')\n",
    "camera_bp.set_attribute('sensor_tick', '0')\n",
    "\n",
    "spawn_point = carla.Transform()\n",
    "camera = world.spawn_actor(camera_bp, spawn_point)\n",
    "\n",
    "time.sleep(1)\n",
    "move_spectator_to(camera.get_transform())\n",
    "\n",
    "# count = 0\n",
    "# def handle_image(image):\n",
    "#     global count\n",
    "#     image.save_to_disk(f'output/{count}.png')\n",
    "#     count += 1\n",
    "\n",
    "# camera.listen(lambda image: handle_image(image))\n",
    "\n",
    "time.sleep(10)\n",
    "camera.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's spawn a vehicle and a camera in a particular position and see what the camera sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_position = carla.Transform(carla.Location(x=-34, y=31, z=11), carla.Rotation(pitch=-18, yaw=-170, roll=0))\n",
    "\n",
    "camera = spawn_camera(transform=camera_position)\n",
    "vehicle = spawn_vehicle()\n",
    "\n",
    "camera.listen(lambda image: image.save_to_disk('output/camera.png'))\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "camera.destroy()\n",
    "vehicle.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a camera to a vehicle\n",
    "\n",
    "In order to acquire images from the point of view of a vehicle, we can attach a camera to it and retrieve what it sees, based on the camera's position and orientation. When the camera is attached to a vehicle, it will move with the vehicle itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "camera.listen(lambda image: image.save_to_disk(f'output/{image.frame}.png'))\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "camera.destroy()\n",
    "vehicle.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many images have been captured by the camera so far?\n",
    "\n",
    "### Live camera feed\n",
    "\n",
    "We can also visualize the camera feed in real-time. This is useful for debugging and understanding what the camera sees.\n",
    "\n",
    "We can use the `opencv` library to create a window and display the camera feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = spawn_vehicle()\n",
    "camera = spawn_camera(attach_to=vehicle)\n",
    "\n",
    "video_output = np.zeros((600, 800, 4), dtype=np.uint8)\n",
    "def camera_callback(image):\n",
    "    global video_output\n",
    "    video_output = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "\n",
    "camera.listen(lambda image: camera_callback(image))\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "cv2.namedWindow('RGB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        cv2.imshow('RGB Camera', video_output)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.destroy()\n",
    "    vehicle.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Multi-camera setup\n",
    "\n",
    "This exercise involves creating a multi-camera setup in the CARLA simulator to simulate a car's mirror and driver point-of-view system using four different cameras. The objective is to capture video streams from different perspectives around a vehicle, mimicking how a driver would view their surroundings through the front windshield, side mirrors, and rearview mirror. You should position each cameras with specific transformations and resolutions, and display each video in individual OpenCV windows.\n",
    "\n",
    "This is an example of output:\n",
    "\n",
    "![Multi-camera setup](img/camera-position-exercise-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Automatic light control\n",
    "\n",
    "Implement a script that allow the vehicle to turn on its lights when it gets dark and turn them off when it gets bright, using the camera sensor.\n",
    "\n",
    "To change the weather conditions, we can use the `PythonAPI\\util\\config.py` script.\n",
    "\n",
    "```bash\n",
    "python ./config.py --weather ClearNoon\n",
    "python ./config.py --weather ClearNight\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Pacman effect using GPS\n",
    "\n",
    "Implement the Pacman effect for a vehicle using a GPS sensor for tracking the vehicle's position. You can start from the implementation of the pacman effect using the vehicle's position below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = spawn_vehicle(world)\n",
    "\n",
    "ROAD_X_MIN = -110\n",
    "ROAD_X_MAX = 110\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        transform = vehicle.get_transform()\n",
    "        location = transform.location\n",
    "\n",
    "        print(f\"Location: {location.x}, {location.y}\", end='\\r')\n",
    "\n",
    "        if location.x > ROAD_X_MAX:\n",
    "            location.x = ROAD_X_MIN\n",
    "            vehicle.set_transform(carla.Transform(location, transform.rotation))\n",
    "\n",
    "        control = carla.VehicleControl(throttle=0.5)\n",
    "        vehicle.apply_control(control)\n",
    "\n",
    "        world.tick()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    vehicle.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Emergency braking system using LIDAR\n",
    "\n",
    "LIDAR (Light Detection and Ranging) is a method for determining ranges by targeting an object or a surface with a laser and measuring the time for the reflected light to return to the receiver.\n",
    "\n",
    "LIDAR sensors are used in autonomous vehicles to detect obstacles and create a 3D representation of the environment.\n",
    "\n",
    "Implment a script that detect obstacles in front of the vehicle and apply the brakes if the distance is less than a threshold, using the LIDAR sensor.\n",
    "\n",
    "Some notes on the simulation environment to test the emergency braking system:\n",
    "\n",
    "- Spawn a vehicle (i.e ego vehicle)\n",
    "- Spawn a vehicle (i.e target vehicle) an position it in front of the ego vehicle\n",
    "- Attach a LIDAR sensor to the ego vehicle\n",
    "- Create the logic to detect obstacles in front of the ego vehicle, based on the LIDAR sensor data. You can find the documentation for the LIDAR sensor [here](https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor).\n",
    "- Apply the brakes if the distance is less than a threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-svs-lab04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
